{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiemet with Long Context Window Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import anthropic\n",
    "from IPython.display import display, Image, Markdown\n",
    "import PIL.Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pdf2image import convert_from_path\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a PDF file and convert it to an image for Gemini\n",
    "def load_pdf_for_llm(pdf_path):\n",
    "    \"\"\"\n",
    "    Load a PDF file from local path and prepare it for Gemini Vision model.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the local PDF file\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: The first page of the PDF as an image\n",
    "    \"\"\"\n",
    "    try:    \n",
    "        # Convert the first page of the PDF to an image\n",
    "        # Convert all pages of the PDF to images\n",
    "        images = convert_from_path(pdf_path)\n",
    "        \n",
    "        if images:\n",
    "            return images\n",
    "        else:\n",
    "            print(\"No images extracted from the PDF.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_for_anthropic(images):\n",
    "    image_content = []\n",
    "    for i, image in enumerate(images):\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format = \"JPEG\")\n",
    "        encoded_image = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "        image_content.append(\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"Image {i+1}\"\n",
    "            }\n",
    "        )\n",
    "        image_content.append(\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"source\": {\n",
    "                    \"type\": \"base64\",\n",
    "                    \"media_type\": \"image/jpeg\",\n",
    "                    \"data\": encoded_image\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    return image_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_empty_graph.pdf',\n",
       " '2_screenshot_text_and_image.pdf',\n",
       " '3_complex_graph.pdf',\n",
       " '4_syllabus.pdf',\n",
       " '5_table.pdf',\n",
       " '6_apple_10k_long_documents.pdf',\n",
       " 'hand_written_math.pdf',\n",
       " 'long_lecture_slide.pdf']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_dir = \"pdf_collection\"\n",
    "pdf_files =sorted(os.listdir(pdf_dir))\n",
    "pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF file: 1_empty_graph.pdf\n",
      "Number of images extracted: 1\n",
      "Processing PDF file: 2_screenshot_text_and_image.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(83235) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83236) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83237) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83238) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83239) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83240) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images extracted: 1\n",
      "Processing PDF file: 3_complex_graph.pdf\n",
      "Number of images extracted: 1\n",
      "Processing PDF file: 4_syllabus.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(83241) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83242) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83243) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83244) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83245) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83246) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images extracted: 9\n",
      "Processing PDF file: 5_table.pdf\n",
      "Number of images extracted: 1\n",
      "Processing PDF file: 6_apple_10k_long_documents.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(83247) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83248) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83249) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83250) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83251) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83252) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images extracted: 121\n",
      "Processing PDF file: hand_written_math.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(83258) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83259) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83261) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images extracted: 7\n",
      "Processing PDF file: long_lecture_slide.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(83264) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83265) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(83266) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images extracted: 68\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for pdf_file in pdf_files:\n",
    "    print(\"Processing PDF file:\", pdf_file)\n",
    "    images = load_pdf_for_llm(os.path.join(pdf_dir, pdf_file))\n",
    "    if images:\n",
    "        print(\"Number of images extracted:\", len(images))\n",
    "    else:\n",
    "        print(\"No images extracted from the PDF.\")\n",
    "    files.append(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.PpmImagePlugin.PpmImageFile"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(files[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Empty_Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini-2.5-pro-exp-03-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's break down this graph:\n",
       "\n",
       "1.  **Type of Graph:** This is a **pie chart**. Pie charts are used to show proportions of a whole. The entire circle represents 100% of something.\n",
       "\n",
       "2.  **Sections/Slices:** The chart is divided into **three sections** (or slices), each represented by a different color:\n",
       "    *   Blue\n",
       "    *   Red (or reddish-orange)\n",
       "    *   Yellow (or gold)\n",
       "\n",
       "3.  **Proportions (Visual Estimation):**\n",
       "    *   The **blue section is the largest**, appearing to represent slightly more than half (maybe around 55-60%) of the total.\n",
       "    *   The **yellow section is the next largest**, looking like roughly a quarter (perhaps 20-25%) of the total.\n",
       "    *   The **red section is the smallest**, appearing slightly smaller than the yellow section (maybe 15-20%).\n",
       "\n",
       "4.  **Missing Information:**\n",
       "    *   **Labels/Legend:** There are no labels or a legend to tell us what each colored section represents (e.g., categories, groups, survey responses).\n",
       "    *   **Values:** No specific percentages or numerical values are shown for each slice.\n",
       "    *   **Title:** There is no title to explain the overall subject of the chart.\n",
       "\n",
       "**In summary:** This is a simple pie chart showing a whole divided into three parts of different sizes. The blue part is the majority, followed by yellow, and then red. Without labels or values, we can only understand the relative proportions visually, not the specific data or context it represents."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro-exp-03-25\",\n",
    "    contents=[\"tell me about this graph?\", files[0]])\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini-2.0-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The image is a pie chart. It is divided into three segments: blue, red, and yellow. The blue segment takes up the largest portion of the pie, followed by red, and then yellow. Without numerical labels or percentages, it's impossible to determine the exact proportions of each segment, but we can visually estimate that blue is probably around 50%, red is around 25-30%, and yellow is around 20-25%."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"tell me about this graph?\", files[0]])\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claude 3.5 Haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This image is a pie chart with four sections in different colors:\n",
       "\n",
       "1. Blue (largest section): Appears to cover approximately 50-60% of the total pie chart area\n",
       "2. Red (medium section): Covers roughly 20-25% of the chart\n",
       "3. Yellow (smaller section): Takes up about 10-15% of the chart\n",
       "4. Blue (smallest section): A small wedge at the top of the chart\n",
       "\n",
       "The colors are bright and solid, and the chart has a clean, simple design. Without additional context, I can't specify what data these sections represent. Pie charts are typically used to show proportions or percentages of a whole, with each slice representing a different category or segment.\n",
       "\n",
       "Would you like me to elaborate on any aspect of the chart?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_content = load_image_for_anthropic(files[0])\n",
    "image_content.append({\"type\": \"text\", \"text\": \"tell me about this graph?\"})\n",
    "\n",
    "response = anthropic_client.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\", \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": image_content\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096\n",
    ")\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Screenshot_Text_and_Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini-2.5-pro-exp-03-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This image is a **promotional graphic or advertisement banner**.\n",
       "\n",
       "Here's a breakdown of its content:\n",
       "\n",
       "1.  **Headline:** \"Sample our Education Journals\" - Clearly states the subject matter being promoted.\n",
       "2.  **Call to Action:** \">> Sign in here to start your access to the latest two volumes for 14 days\".\n",
       "    *   \"Sign in here\" is highlighted (likely a hyperlink in a digital context).\n",
       "    *   It specifies the benefit: access to the \"latest two volumes\".\n",
       "    *   It details the duration of the offer: \"for 14 days\".\n",
       "3.  **Visual Elements:**\n",
       "    *   A solid blue background.\n",
       "    *   White text for readability.\n",
       "    *   A white icon in the top right corner, which appears to be a stylized oil lamp or \"lamp of knowledge,\" commonly associated with education and learning.\n",
       "\n",
       "In summary, it's an **advertisement offering a 14-day free trial access to recent education journals**, requiring users to sign in to activate the trial."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro-exp-03-25\",\n",
    "    contents=[\"Describe this content?\", files[1]])\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini-2.0-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The image is an advertisement on a blue background. The text on the image reads \"Sample our Education Journals.\" Below that, it says \">> Sign in here to start your access to the latest two volumes for 14 days\". There is also a small icon of an oil lamp on the top right of the image."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro-exp-03-25\",\n",
    "    contents=[\"Describe this content?\", files[1]])\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claude 3.5 Haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This image is a sign-in page or banner for \"Sample our Education Journals\". The banner is in a blue color scheme with white text. It invites users to \"Sign In here to start your access to the latest two volumes for 14 days\". There's a small white icon of what appears to be a bowl or cup in the top right corner of the banner. The text suggests this is a trial or preview access to educational journal volumes, allowing users 14 days of complimentary access after signing in."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_content = load_image_for_anthropic(files[1])\n",
    "image_content.append({\"type\": \"text\", \"text\": \"Describe this content?\"})\n",
    "\n",
    "response = anthropic_client.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\", \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": image_content\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096\n",
    ")\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Complex_Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini-2.5-pro-exp-03-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Figure 2 illustrates the architecture of the **Pixtral Vision Encoder**.\n",
       "\n",
       "Here's a breakdown of the components shown:\n",
       "\n",
       "1.  **Image Patches:** The input starts with image patches from potentially multiple images with variable sizes and aspect ratios.\n",
       "2.  **RoPE-2D:** Rotary Position Embeddings (2D) are applied to the patches to handle variable image sizes.\n",
       "3.  **Block-diagonal attention mask:** This mask is applied to enable sequence packing, allowing efficient batching of images/patches while preventing attention leakage between different images in the batch.\n",
       "4.  **Pixtral-ViT:** The core Vision Transformer component, specifically adapted for the Pixtral model (indicated as a Bidirectional transformer).\n",
       "5.  **Vision-Language Projector:** A layer that likely maps the visual features extracted by the ViT into a space compatible with a language model.\n",
       "6.  **MLP:** A Multi-Layer Perceptron for further processing.\n",
       "7.  **Output Embeddings:** The final output representation of the processed image patches.\n",
       "\n",
       "The diagram also visually represents how sequence packing works using block-diagonal attention and includes special tokens like `[IMG_BREAK]` and `[IMG_END]`. The overall purpose is to show how Pixtral encodes images, particularly focusing on its ability to handle variable input sizes and efficient batching."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro-exp-03-25\",\n",
    "    contents=[\"What's inside Figure 2?\", files[2]])\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini-2.0-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Figure 2 illustrates the Pixtral Vision Encoder. It shows the flow of information through the encoder, starting with Image Patches, then RoPE-2D (position encodings), a Block-diagonal attention mask, a Bidirectional Transformer, Pixtral-ViT, a Vision-Language Projector, an MLP, and finally, Output Embeddings. The figure also shows that \"Add [IMG_BREAK] and [IMG_END] tokens\" is performed after the Vision-Language Projector.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"What's inside Figure 2?\", files[2]])\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claude 3.5 Haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Figure 2 shows the architecture of the Pixtral Vision Encoder. It's a detailed diagram that illustrates the components and data flow of the vision encoding process. The figure includes:\n",
       "\n",
       "1. A block diagram with multiple layers, including:\n",
       "- Output Encoding\n",
       "- Vision-Language Projector\n",
       "- Pixtral-VIT (Vision Transformer)\n",
       "\n",
       "2. Color-coded blocks representing different stages of image processing\n",
       "\n",
       "3. Small visual representations on the right side, including:\n",
       "- A pie chart\n",
       "- A histogram or heatmap-like image\n",
       "\n",
       "The caption explains that Pixtral uses a new vision encoder trained from scratch to natively support variable image sizes and aspect ratios. It highlights features like block-diagonal attention masks and sequence packing for batching, and notes that the encoder facilitates variable image sizes while supporting 2D encodings.\n",
       "\n",
       "The diagram provides a visual breakdown of the technical architecture and data transformation process of the Pixtral Vision Encoder."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_content = load_image_for_anthropic(files[2])\n",
    "image_content.append({\"type\": \"text\", \"text\": \"What's inside Figure 2?\"})\n",
    "\n",
    "response = anthropic_client.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\", \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": image_content\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096\n",
    ")\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Syllabus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini-2.5-pro-exp-03-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the syllabus provided (page 2, under \"Weekly Structure\"), **Week 2** is part of the block covering **SQL Language**.\n",
       "\n",
       "This block spans Weeks 2-4 and includes the following topics (as detailed on page 3):\n",
       "*   Introduction to SQL\n",
       "*   Intermediate SQL\n",
       "*   Advanced SQL\n",
       "\n",
       "Therefore, Week 2 will focus on the initial parts of learning SQL Language, likely starting with the \"Introduction to SQL\"."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro-exp-03-25\",\n",
    "    contents=[\"What's covered in week 2?\", files[3]])\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini-2.0-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Weeks 2-4 covers SQL language, including:\n",
       "\n",
       "*   Introduction to SQL\n",
       "*   Intermediate SQL\n",
       "*   Advanced SQL"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"What's covered in week 2?\", files[3]])\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claude 3.5 Haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the course syllabus (Image 3), in Weeks 2-4, the course will cover SQL Language. More specifically, the topics for SQL Language include:\n",
       "\n",
       "- Introduction to SQL\n",
       "- Intermediate SQL\n",
       "- Advanced SQL"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_content = load_image_for_anthropic(files[3])\n",
    "image_content.append({\"type\": \"text\", \"text\": \"What's covered in week 2?\"})\n",
    "\n",
    "response = anthropic_client.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\", \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": image_content\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096\n",
    ")\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini-2.5-pro-exp-03-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here is the table extracted from the image:\n",
       "\n",
       "| Category/Label                    | Percentage % | Count |\n",
       "| :-------------------------------- | :----------- | :---- |\n",
       "| **Gender**                        |              |       |\n",
       "| Female                            | 49.0%        | 540   |\n",
       "| Male                              | 51.0%        | 561   |\n",
       "| Early Childhood                   | 3.3%         | 36    |\n",
       "| **What grades do you teach in school?** |              |       |\n",
       "| Elementary School                 | 21.0%        | 231   |\n",
       "| High School                       | 29.4%        | 324   |\n",
       "| Primary School                    | 46.3%        | 510   |\n",
       "| **Educational Levels**            |              |       |\n",
       "| Bachelor                          | 83.1%        | 915   |\n",
       "| Master                            | 12.0%        | 132   |\n",
       "| PhD                               | 2.7%         | 30    |\n",
       "| Diploma                           | 2.2%         | 24    |\n",
       "| **Teaching Experience**           |              |       |\n",
       "| 0–5                               | 24.3%        | 267   |\n",
       "| 11–15                             | 36.8%        | 405   |\n",
       "| 6–10                              | 37.6%        | 414   |\n",
       "| +16                               | 1.4%         | 15    |\n",
       "\n",
       "**Note:** The item \"Early Childhood\" appears structurally under \"Gender\" in the original image, but contextually might relate more to the \"Grades Taught\" section. Similarly, \"Bachelor\" appears before the \"Educational Levels\" heading but clearly belongs to that category. The table above reflects the data points and structure as presented, grouping \"Bachelor\" under \"Educational Levels\" for clarity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro-exp-03-25\",\n",
    "    contents=[\"Extract the table from this image and present it in a table format\", files[4]])\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini-2.0-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is the table extracted from the image:\n",
       "\n",
       "| Category                    |   Percentage % |   Count |\n",
       "|:----------------------------|---------------:|--------:|\n",
       "| **Gender**                  |                |         |\n",
       "| Female                      |           49   |     540 |\n",
       "| Male                        |           51   |     561 |\n",
       "| Early Childhood             |            3.3 |      36 |\n",
       "| **What grades do you teach in school?**       |                |         |\n",
       "| Elementary School           |           21   |     231 |\n",
       "| High School                 |           29.4 |     324 |\n",
       "| Primary School              |           46.3 |     510 |\n",
       "| Bachelor                    |           83.1 |     915 |\n",
       "| **Educational Levels**      |                |         |\n",
       "| Master                      |           12   |     132 |\n",
       "| PhD                         |            2.7 |      30 |\n",
       "| Diploma                     |            2.2 |      24 |\n",
       "| **Teaching Experience**     |                |         |\n",
       "| 0-5                         |           24.3 |     267 |\n",
       "| 11-15                       |           36.8 |     405 |\n",
       "| 6-10                        |           37.6 |     414 |\n",
       "| +16                         |            1.4 |      15 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"Extract the table from this image and present it in a table format\", files[4]])\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claude 3.5 Haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's the table extracted from the image:\n",
       "\n",
       "| Category | Percentage % | Count |\n",
       "|---|---|---|\n",
       "| **Gender** |  |  |\n",
       "| Female | 49.0% | 540 |\n",
       "| Male | 51.0% | 561 |\n",
       "| Early Childhood | 3.3% | 36 |\n",
       "| **What grades do you teach in school?** |  |  |\n",
       "| Elementary School | 21.0% | 231 |\n",
       "| High School | 29.4% | 324 |\n",
       "| Primary School | 46.3% | 510 |\n",
       "| **Educational Levels** |  |  |\n",
       "| Bachelor | 83.1% | 915 |\n",
       "| Master | 12.0% | 132 |\n",
       "| PhD | 2.7% | 30 |\n",
       "| Diploma | 2.2% | 24 |\n",
       "| **Teaching Experience** |  |  |\n",
       "| 0-5 | 24.3% | 267 |\n",
       "| 11-15 | 36.8% | 405 |\n",
       "| 6-10 | 37.6% | 414 |\n",
       "| +16 | 1.4% | 15 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_content = load_image_for_anthropic(files[4])\n",
    "image_content.append({\"type\": \"text\", \"text\": \"Extract the table from this image and present it in a table format\"})\n",
    "\n",
    "response = anthropic_client.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\", \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": image_content\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096\n",
    ")\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6: Apple 10K - Long Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini-2.5-pro-exp-03-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided Apple Inc. Form 10-K for the fiscal year ended September 28, 2024, the product, service, and software announcements during that year are listed on page 21 under \"Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\".\n",
       "\n",
       "Here are the significant announcements mentioned for fiscal year 2024:\n",
       "\n",
       "**First Quarter 2024:**\n",
       "\n",
       "*   MacBook Pro 14-in.\n",
       "*   MacBook Pro 16-in.\n",
       "*   iMac\n",
       "\n",
       "**Second Quarter 2024:**\n",
       "\n",
       "*   MacBook Air 13-in.\n",
       "*   MacBook Air 15-in.\n",
       "\n",
       "**Third Quarter 2024:**\n",
       "\n",
       "*   iPad Air\n",
       "*   iPad Pro\n",
       "*   iOS 18, macOS Sequoia, iPadOS 18, watchOS 11, visionOS 2 and tvOS 18 (updates to the Company’s operating systems)\n",
       "*   Apple Intelligence™ (a personal intelligence system that uses generative models)\n",
       "\n",
       "**Fourth Quarter 2024:**\n",
       "\n",
       "*   iPhone 16, iPhone 16 Plus, iPhone 16 Pro and iPhone 16 Pro Max\n",
       "*   Apple Watch Series 10\n",
       "*   AirPods 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro-exp-03-25\",\n",
    "    contents=[\"What are the product, service and software annoucnements?\", files[5]])\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"answer\": {\n",
       "    \"Americas\": {\n",
       "      \"Net sales (millions)\": 167045,\n",
       "      \"Operating income (millions)\": 67656\n",
       "    },\n",
       "    \"Europe\": {\n",
       "      \"Net sales (millions)\": 101328,\n",
       "      \"Operating income (millions)\": 41790\n",
       "    },\n",
       "    \"Greater China\": {\n",
       "      \"Net sales (millions)\": 66952,\n",
       "      \"Operating income (millions)\": 27082\n",
       "    },\n",
       "    \"Japan\": {\n",
       "      \"Net sales (millions)\": 25052,\n",
       "      \"Operating income (millions)\": 12454\n",
       "    },\n",
       "    \"Rest of Asia Pacific\": {\n",
       "      \"Net sales (millions)\": 30658,\n",
       "      \"Operating income (millions)\": 13062\n",
       "    }\n",
       "  },\n",
       "  \"page_number\": 47\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro-exp-03-25\",\n",
    "    contents=[\"What's the segment operating performance for 2024, by geography? Provide the answer in JSON with two keys: answer and page_number}\", files[5]])\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"answer\": \"From 2023 to 2024, Products net sales decreased from $298,085 million to $294,866 million. Services net sales increased from $85,200 million in 2023 to $96,169 million in 2024.\",\n",
       "  \"page_number\": 23\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro-exp-03-25\",\n",
    "    contents=[\"How has products and servcies revenue changed from 2023 to 2024? Provide the answer in JSON with two keys: answer and page_number}\", files[5]])\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini-2.0-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here are the product, service, and software announcements made by Apple during the fiscal year of 2024 according to the 10-K report:\n",
       "\n",
       "**First Quarter 2024:**\n",
       "\n",
       "*   MacBook Pro 14-in.\n",
       "*   MacBook Pro 16-in.\n",
       "*   iMac\n",
       "\n",
       "**Second Quarter 2024:**\n",
       "\n",
       "*   MacBook Air 13-in.\n",
       "*   MacBook Air 15-in.\n",
       "\n",
       "**Third Quarter 2024:**\n",
       "\n",
       "*   iPad\n",
       "*   iPad Pro\n",
       "*   iOS 18, macOS Sequoia, iPadOS 18, watchOS 11, visionOS 2 and tvOS 18, updates to the Company’s operating systems\n",
       "*   Apple Intelligence™, a personal intelligence system that uses generative models\n",
       "\n",
       "**Fourth Quarter 2024:**\n",
       "\n",
       "*   iPhone 16, iPhone 16 Plus, iPhone 16 Pro and iPhone 16 Pro Max\n",
       "*   Apple Watch Series 10\n",
       "*   AirPods 4\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"What are the product, service and software annoucnements?\", files[5]])\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"answer\": \"{\\n \\\"Americas\\\": {\\n \\\"Net sales\\\": \\\"167,045\\\",\\n \\\"Operating income\\\": \\\"67,656\\\"\\n },\\n \\\"Europe\\\": {\\n \\\"Net sales\\\": \\\"101,328\\\",\\n \\\"Operating income\\\": \\\"41,790\\\"\\n },\\n \\\"Greater China\\\": {\\n \\\"Net sales\\\": \\\"66,862\\\",\\n \\\"Operating income\\\": \\\"27,002\\\"\\n },\\n \\\"Japan\\\": {\\n \\\"Net sales\\\": \\\"25,652\\\",\\n \\\"Operating income\\\": \\\"12,454\\\"\\n },\\n \\\"Rest of Asia Pacific\\\": {\\n \\\"Net sales\\\": \\\"30,658\\\",\\n \\\"Operating income\\\": \\\"13,052\\\"\\n }\\n}\",\n",
       "  \"page_number\": 47\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"What's the segment operating performance for 2024, by geography? Provide the answer in JSON with two keys: answer and page_number}\", files[5]])\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"answer\": \"Services net sales increased during 2024 compared to 2023 due primarily to higher net sales from advertising, the App Store, and cloud services.\",\n",
       "  \"page_number\": 23\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"How has products and servcies revenue changed from 2023 to 2024? Provide the answer in JSON with two keys: answer and page_number}\", files[5]])\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claude 3.5 Haiku (Aborted due to context window limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_content = load_image_for_anthropic(files[5])\n",
    "image_content.append({\"type\": \"text\", \"text\": \"What are the product, service and software annoucnements?\"})\n",
    "\n",
    "response = anthropic_client.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\", \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": image_content\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096\n",
    ")\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_content.append({\"type\": \"text\", \"text\": \"What's the segment operating performance for 2024, by geography? Provide the answer in JSON with two keys: answer and page_number}\"})\n",
    "\n",
    "response = anthropic_client.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\", \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": image_content\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096\n",
    ")\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_content.append({\"type\": \"text\", \"text\": \"What are the product, service and software annoucnements?\"})\n",
    "\n",
    "response = anthropic_client.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\", \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": image_content\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096\n",
    ")\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = gemini_client.caches.create(\n",
    "    model = \"gemini-2.0-flash\",\n",
    "    config = types.CreateCachedContentConfig(\n",
    "        display_name = \"apple_10k_cache\",\n",
    "        system_instruction = (\n",
    "            \"You are a financial analyst that can answer questions about the Apple 10K report.\"\n",
    "        ),\n",
    "        contents = [files[2]],\n",
    "        ttl = \"600s\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the product, service, and software announcements from the 10K report:\n",
       "\n",
       "**First Quarter 2024:**\n",
       "\n",
       "*   MacBook Pro 14-in.\n",
       "*   MacBook Pro 16-in.\n",
       "*   iMac\n",
       "\n",
       "**Second Quarter 2024:**\n",
       "\n",
       "*   MacBook Air 13-in.\n",
       "*   MacBook Air 15-in.\n",
       "\n",
       "**Third Quarter 2024:**\n",
       "\n",
       "*   iPad\n",
       "*   iPad Pro\n",
       "*   iOS 18, macOS Sequoia, iPadOS 18, watchOS 11, visionOS 2 and tvOS 18, updates to the Company's operating systems\n",
       "*   Apple Intelligence, a personal intelligence system that uses generative models\n",
       "\n",
       "**Fourth Quarter 2024:**\n",
       "\n",
       "*   iPhone 16, iPhone 16 Plus, iPhone 16 Pro and iPhone 16 Pro Max\n",
       "*   Apple Watch Series 10\n",
       "*   AirPods 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"What are the product, service and software annoucnements?\"],\n",
    "    config = types.GenerateContentConfig(\n",
    "        cached_content = cache.name,\n",
    "        temperature = 0.0,\n",
    "    )\n",
    ")\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "\"answer\": \"Americas: $167,045\\nEurope: $101,328\\nGreater China: $66,652\\nJapan: $25,652\\nRest of Asia Pacific: $30,658\\nTotal net sales: $391,335\",\n",
       "\"page_number\": 22\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"What's the segment operating performance for 2024, by geography? Provide the answer in JSON with two keys: answer and page_number}\"],\n",
    "    config = types.GenerateContentConfig(\n",
    "        cached_content=cache.name,\n",
    "        temperature = 0.0,\n",
    "    )\n",
    ")\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"answer\": \"From 2023 to 2024, iPhone revenue decreased from $200,563 million to $201,183 million. Mac revenue increased from $29,350 million to $29,984 million. iPad revenue decreased from $26,850 million to $25,694 million. Wearables, Home and Accessories revenue decreased from $39,840 million to $37,005 million. Services revenue increased from $85,250 million to $96,169 million.\",\n",
       "  \"page_number\": 23\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model = \"gemini-2.0-flash\",\n",
    "    contents = [\"How has products and servcies revenue changed from 2023 to 2024? Provide the answer in JSON with two keys: answer and page_number}\"],\n",
    "    config = types.GenerateContentConfig(\n",
    "        cached_content = cache.name\n",
    "    )\n",
    ")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- Caching itself takes quite some time\n",
    "- Cashing costs: \n",
    "    - $1.00/1,000,000 tokens per hour\n",
    "    - the cached tokens are then billed at $0.025/1,000,000 tokens, 1/4 of the original cost ($0.1/1,000,000 tokens) when used in following prompts\n",
    "- The minimum input token count for context caching is 4,096, and the maximum is the same as the maximum for the given model\n",
    "- For cashed documents, the reponse time is roughly 1/4 the time of the non-cached responses\n",
    "\n",
    "### Quick Cost Estimate\n",
    "- Assumptions: 30 page lecture slides, 60,000 tokens, 30 minutes study session, 20 questions\n",
    "- No-Caching Cost: 60,000 * 20 / 1,000,000 * 0.1 = $0.12\n",
    "- Caching Cost: 60,000 / 1,000,000 * 0.5 * 1 + 60,000 * 20 / 1,000,000 * 0.025 = $0.06\n",
    "- Intuition: longer caching time makes caching less cost-effective, more questions asked makes caching more cost-effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
